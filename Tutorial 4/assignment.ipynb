{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from os import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#import autograd\n",
    "from math import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Exercise in $\\mathbb{R}^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given the following optimization problem:\n",
    "\n",
    "\\begin{align}\n",
    "&\\underset{x}\\min & & x_1 x_2 + x_2 x_3 \\\\\n",
    "& \\text{s.t.} & & x_1^2 + x_2^2 - 2 = 0 \\\\ \n",
    "& & & x_1^2 + x_3^2 - 2 = 0\n",
    "\\end{align}\n",
    "\n",
    "Let $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} \\in \\mathbb{R}^3$. We are to minimize $f(\\mathbf{x}) = x_1 x_2 + x_2 x_3$ subject to the constraint: $\\mathbf{h}(\\mathbf{x}) = \\begin{pmatrix} h_1(\\mathbf{x}) \\\\ h_2(\\mathbf{x}) \\end{pmatrix} = \\begin{pmatrix} x_1^2 + x_2^2 - 2 \\\\ x_1^2 + x_3^2 - 2 \\end{pmatrix} = \\mathbf{0}$.\n",
    "\n",
    "We form the Lagrangian of the above problem as follows:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{x}, \\mathbf{\\lambda}) = f(\\mathbf{x}) + \\lambda^\\top \\mathbf{h}(\\mathbf{x}) = x_1 x_2 + x_2 x_3 + \\lambda_1 (x_1^2 + x_2^2 - 2) + \\lambda_2 (x_2^2 + x_3^2 - 2)$$\n",
    "\n",
    "where $\\mathbf{\\lambda} = \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\end{pmatrix} \\in \\mathbb{R^2}$. \n",
    "\n",
    "The first order optimality conditions are given by: $\\begin{cases} \\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\mathbf{\\lambda}) = 0 \\\\ \\nabla_{\\mathbf{\\lambda}} \\mathcal{L}(\\mathbf{x}, \\mathbf{\\lambda}) = 0 \\end{cases}$. Solving for the 2 gradients, we get: \n",
    "\n",
    "$\\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\mathbf{\\lambda}) = \\begin{pmatrix} x_2 \\\\ x_1 + x_3 \\\\ x_2 \\end{pmatrix} + \\lambda_1 \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\\\ 0 \\end{pmatrix} + \\lambda_2 \\begin{pmatrix} 0 \\\\ 2x_2 \\\\ 2x_3\\end{pmatrix} = \\begin{pmatrix} x_2 + 2\\lambda_1 x_1 \\\\ x_1 + x_3 + 2\\lambda_1 x_2 + 2\\lambda_2 x_2 \\\\ x_2 + 2\\lambda_2 x_3 \\end{pmatrix} = \\mathbf{0}.$\n",
    "\n",
    "$\\nabla_{\\mathbf{\\lambda}} \\mathcal{L}(\\mathbf{x}, \\mathbf{\\lambda}) = \\begin{pmatrix} x_1^2 + x_2^2 - 2 \\\\ x_2^2 + x_3^2 - 2 \\end{pmatrix} = \\mathbf{0}$.\n",
    "\n",
    "This yields the following system of non-linear equations to be solved:\n",
    "\n",
    "$\\begin{cases} x_2 &+ &2\\lambda_1 x_1 &&&& & = 0 \\\\ x_1& + &x_3 &+ &2\\lambda_1 x_2 &+& 2\\lambda_2 x_2 &= 0 \\\\ x_2& +& 2\\lambda_2 x_3&&&& &= 0\\\\ x_1^2 &+& x_2^2 &- &2&& &= 0\\\\ x_2^2& +& x_3^2 &-& 2&&& = 0 \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the system and the corresponding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computational reasons and code cleanness, we consider $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\lambda_1 \\\\ \\lambda_2 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "# symbic function and its jacobian\n",
    "x1 = sp.Symbol('x1')\n",
    "x2 = sp.Symbol('x2')\n",
    "x3 = sp.Symbol('x3')\n",
    "lamda1 = sp.Symbol('lamda1')\n",
    "lamda2 = sp.Symbol('lamda2')\n",
    "x = np.array([x1, x2,x3,lamda1,lamda2])\n",
    "g_Lx1=x[1]-2*lamda1*x[0]-2*lamda2*x[0]\n",
    "g_Lx2=x[0]+x[2]-2*lamda1*x[1]\n",
    "g_Lx3=x[1]-2*lamda2*x[2]\n",
    "g_L_lam1=x1**2 + x2**2 - 2\n",
    "g_L_lam2= x1**2 + x3**2 - 2\n",
    "\n",
    "F = sp.Matrix([g_Lx1,g_Lx2,g_Lx3,g_L_lam1,g_L_lam2])\n",
    "#F = sp.Matrix([f_symb,g_symb1,g_symb2])\n",
    "J=F.jacobian(x)\n",
    "#J=sp.derive_by_array(F,x)\n",
    "#Eq = np.transpose(np.transpose(lam)*J)\n",
    "#Eq = np.transpose(lam)*J\n",
    "\n",
    "F_true=sp.lambdify([x], F, np)\n",
    "J_true=sp.lambdify([x], J, np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix([\n",
       "[-2*lamda1 - 2*lamda2,         1,         0, -2*x1, -2*x1],\n",
       "[                   1, -2*lamda1,         1, -2*x2,     0],\n",
       "[                   0,         1, -2*lamda2,     0, -2*x3],\n",
       "[                2*x1,      2*x2,         0,     0,     0],\n",
       "[                2*x1,         0,      2*x3,     0,     0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J\n",
    "#Full_Eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1,1,0.5,0.1,0.2])\n",
    "np.transpose(J_true(a)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globalized newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merit_func(f,x):\n",
    "    return .5*np.transpose(f(x))@f(x).reshape(-1)\n",
    "\n",
    "def backtrack_linesearch(F_true,merit_func, gk, pk, xk, alpha = 0.1, beta = 0.8):\n",
    "    t = 1\n",
    "    while ( merit_func(F_true,xk + t*pk)[0] >= merit_func(F_true,xk)[0] + gk @ pk*alpha * t  ):\n",
    "        t *= beta\n",
    "    return t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1622776601683795\n",
      "0.5796011559684817\n",
      "0.08432770501379501\n",
      "0.0008563365753152141\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a=np.array([1,1,1,1,1])\n",
    "history=a\n",
    "while(la.norm(F_true(a)) > 1e-6):\n",
    "    print(la.norm(F_true(a)))\n",
    "    #using svd\n",
    "    inv=la.pinv(np.transpose(J_true(a))@J_true(a).reshape(5,5),rcond=1e-3)\n",
    "    p=-inv@np.transpose(J_true(a))@F_true(a).reshape(-1)\n",
    "    t = backtrack_linesearch(F_true,merit_func,np.transpose(J_true(a))@F_true(a).reshape(-1), p, a)\n",
    "    a=a+t*p\n",
    "    history = np.vstack( (history, a) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nooq+3gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ccedfad208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nsteps = history.shape[0]\n",
    "fhist = np.zeros(nsteps)\n",
    "for i in range(nsteps):\n",
    "    fhist[i] = la.norm(F_true(history[i]))\n",
    "plt.figure()\n",
    "\n",
    "plt.semilogy(np.arange(0, nsteps), fhist, linewidth=1)\n",
    "plt.title('Convergence Performance')\n",
    "plt.xlabel('Iteration count')\n",
    "plt.ylabel('r$|f^k - f^*|$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5411961 , 1.30656297, 1.30656297, 0.70710676, 0.50000002])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Significance of the Lagrange Multipliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the optimization problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{x}\\min\n",
    "& & f(x) \\\\\n",
    "& \\text{s.t.} & &  h_i(x) = 0  \\quad i = 1, \\ldots, p\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We assume this problem has an optimal solution $x^*$. At $x^*$, we have $h_i(x^*) = 0 \\; \\forall i = 1, \\ldots, p$. Assume, the $i^{th}$ constraint is varied by $\\epsilon$. Using Taylor's expansion of $h_i(\\cdot)$ at $x^*$:\n",
    "\n",
    "\\begin{align}\n",
    "h_i(x^* + \\Delta x) & = h_i(x^*) + \\nabla h_i(x^*)^\\top \\Delta x + \\ldots \\\\ \n",
    "& = 0 + \\nabla h_i(x^*)^\\top \\Delta x + \\ldots \\\\ \n",
    "& = \\epsilon \\\\ \n",
    "\\end{align}\n",
    "\n",
    "We thus have $\\nabla h_i(x^*)^\\top \\Delta x = \\epsilon$. We now examine the value of $f(\\cdot)$ a $\\Delta x$ step from $x^*$. Using Taylor's expansion:\n",
    "\n",
    "\\begin{align}\n",
    "f(x^* + \\Delta x) & = f(x^*) + \\nabla f(x^*)^\\top \\Delta x + \\ldots \\\\ \n",
    "\\end{align}\n",
    "\n",
    "We find that, up to an error of $o\\left(\\lVert \\Delta x \\lVert^2\\right)$:\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta f & = f(x^* + \\Delta x) - f(x^*) & \\\\\n",
    "& = \\nabla f(x^*)^\\top \\Delta x & \\\\\n",
    "& = \\left(- \\lambda^{*^\\top} \\nabla \\mathbf{h} (x^*)\\right) ^ \\top \\Delta x & \\\\ \n",
    "& = - \\lambda_i^* \\nabla h_i(x^*)^\\top \\Delta x & \\left(\\nabla h_j^* \\bot \\Delta x \\quad \\forall \\, j \\neq i\\right) \\\\ \n",
    "& = -\\lambda_i^* \\, \\epsilon\n",
    "\\end{align}\n",
    "\n",
    "For infinitesimal change, we get $\\frac{df^*}{d\\epsilon} = -\\lambda_i^*$. This means that if the $i^{th}$ constraint changes by $\\epsilon$, the objective function value will change by $-\\lambda_i^*$, i.e. the Lagrangian multipliers can be thought of as the sensitivities of the objective function relative to the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Nonnegativity of the Lagrange Multipliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the optimization problem be defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{x} \\min & & f(x) \\\\\n",
    "& \\text{s.t.} & & h_i(x) = 0 \\quad i = 1, \\ldots, p \\\\\n",
    "& & & g_i(x) \\leq 0 \\quad i = 1, \\dots, m\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $x\\in \\mathbb{R}^n$, and $f, h_i, g_j: \\mathbb{R}^n \\rightarrow \\mathbb{R} \\quad \\forall \\,i, \\,j$.\n",
    "\n",
    "We define the Lagrangian of the equation as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x; \\mu, \\lambda) = f(x) + \\mu^\\top \\mathbf{h} (x) + \\lambda^\\top \\mathbf{g}(x)\n",
    "$$\n",
    "where $\\mu = \\left[ \\mu_1, \\ldots, \\mu_p \\right]^\\top \\in \\mathbb{R}^p$ and $\\lambda = \\left[ \\lambda_1, \\ldots, \\lambda_m \\right]^\\top \\in \\mathbb{R}_+^m$. In this problem, we will justify the range of the Lagrangian multipliers.\n",
    "\n",
    "In general, for a given optimization problem, there are 2 types of relaxations:\n",
    "- 1\\. Removing one or multiple constraints (assuming the removed constraints are active at the optimal point). This makes the feasible region bigger, and hence introduces the possibility of having a new minimum.\n",
    "- 2\\. Let $\\tilde{f}: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be such that $\\tilde{f}(x) \\leq f(x) \\; \\forall x$. Solving the optimization problem with the new objective $\\tilde{f}$ results in an optimal point $\\tilde{x}^* \\leq x^*$.\n",
    "\n",
    "The Lagrangian equivalent incorporates these 2 relaxations, by removing all constraints, and adding the constraints to the objective function in a way that the new objective function is a relaxed version of the old one. To see this, consider the following. \n",
    "\n",
    "At the optimal point, $\\mathbf{h}(x^*) = \\mathbf{0}$, this means that $f(x^*) + \\mu^\\top \\mathbf{h}(x^*) = f(x^*) + 0 = f(x^*)$. This means that no matter the value of $\\mu$, the optimal value is not changed, and thus $mu$ can take on any real value. \n",
    "Similarly, at the optimal point, we have $\\mathbf{g}(x^*) \\leq \\mathbf{0}$. The new objective value at the optimal point is thus $f(x^*) + \\mu^\\top \\mathbf{h}(x^*) + \\lambda^\\top \\mathbf{g}(x^*) = f(x^*) + 0 + \\lambda^\\top \\mathbf{g}(x^*)$. Since $\\mathbf{g}(x^*) \\leq \\mathbf{0}$, the only way $\\tilde{f}(x^*) \\leq f(x^*)$ is by forcing $\\lambda \\geq 0$.\n",
    "\n",
    "To see it differently, if $\\lambda \\leq 0$, we will have $f(x^*) + \\lambda^\\top \\mathbf{g}(x^*) \\geq f(x^*)$. Solving the new problem will thus result in an optimal value larger than the original optimal value, and thus failing to get the optimal value.\n",
    "\n",
    "To show this, consider the following optimization problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{x}\\min & & x(x-1)(x+2)^2 \\\\\n",
    "& \\text{s.t.} & & 0.25(x-1)^2 - 2 \\leq 0\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "This can be seen as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 - Necessary but not sufficient conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following optimization problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{x} \\min & & x_1 - x_2 \\\\\n",
    "& \\text{s.t.} & & x_1 x_2 + 4 = 0\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Consider also the 2 points $a = [-2, 2]^\\top$ and $b = [2, -2]^\\top$. The Lagrangian is given by:\n",
    "\n",
    "$$\\mathcal{L}(x; \\mu) = x_1 - x_2 + \\mu (x_1 x_2 + 4)$$\n",
    "\n",
    "The first order optimality conditions are:\n",
    "$$\\nabla_x \\mathcal{L} = \\begin{pmatrix} 1 + \\mu x_2 \\\\ -1 + \\mu x_1\n",
    "\\end{pmatrix} = 0$$\n",
    "\n",
    "$$\\nabla_\\mu \\mathcal{L} = x_1 x_2 + 4 = 0$$\n",
    "\n",
    "For point $a = [-2, 2]^\\top$ , we have:\n",
    "\n",
    "$$\\nabla_x \\mathcal{L}(a; \\mu) = \\begin{pmatrix} 1 + 2\\mu \\\\ -1 - 2\\mu\\end{pmatrix} = 0$$\n",
    "\n",
    "giving $\\mu = -\\frac{1}{2}$.\n",
    "\n",
    "$$\\nabla_\\mu \\mathcal{L}(a; \\mu) = -2 \\times 2 + 4 = 0$$\n",
    "\n",
    "Similarly, for point $b = [2, -2]^\\top$ , we have:\n",
    "\n",
    "$$\\nabla_x \\mathcal{L}(b; \\mu) = \\begin{pmatrix} 1 - 2\\mu \\\\ -1 + 2\\mu\\end{pmatrix} = 0$$\n",
    "\n",
    "giving $\\mu = \\frac{1}{2}$.\n",
    "\n",
    "$$\\nabla_\\mu \\mathcal{L} (b; \\mu) = 2 \\times (-2) + 4 = 0$$\n",
    "\n",
    "This shows both $a$ and $b$ satisfy the first order optimality conditions. However neither point is a solution. To see this, let $x_1 = \\frac{-4}{x_2}$. By letting $x_2 \\rightarrow 0^+$, we will have $x_1 \\rightarrow - \\infty$. This means the objective function will tend to $-\\infty$, and the problem is unbounded, admitting no real optimal point.\n",
    "\n",
    "This shows that the first order optimality conditions are necessary for a point to be a feasible solution, however, they are not sufficient so the points that satisfy them be optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 - Healthy Snack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A health conscious buyer needs at least $6$ ounces of chocolate, $10$ ounces of sugar and $8$ ounces of cream cheese. We have 2 products: Brownies ($x_1$) and Cheesecakes ($x_2$), costing $50$ and $80$ cents respectively. The following table shows the contents of each product along with the cost:\n",
    "\n",
    "\\begin{array}{|l|c|c|c|c|}\n",
    "\\hline & Chocolate & Sugar  & Cream \\; Cheese & Cost \\\\ \n",
    "\\hline Brownie \\; (x_1)& 3 & 2 & 2 & 50 \\\\ \\hline\n",
    "Cheesecake \\; (x_2) & 0 & 4 & 5 & 80 \\\\ \\hline\n",
    "\\end{array}\n",
    "\n",
    "We can formulate the following optimization problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{x} \\min & & 50x_1 + 80 x_2 \\\\ \n",
    "& \\text{s.t.} & & 3x_1 \\geq 6 \\\\\n",
    "& & & 2x_1 + 4x_2 \\geq 10 \\\\ \n",
    "& & & 2x_1 + 5x_2 \\geq 8\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Let $c = \\begin{pmatrix} 50 \\\\ 80 \\end{pmatrix}$, $\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^+$, $A = \\begin{pmatrix} -3 & 0 \\\\ -2 & -4 \\\\ -2 & -5 \\end{pmatrix}$ and $b = \\begin{pmatrix} -6 \\\\ -10 \\\\ -8 \\end{pmatrix}$.\n",
    "\n",
    "We can then reformulate the problem as follows (form required by $\\textbf{ linprog}$):\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{x} \\min & & c^\\top x \\\\ \n",
    "& \\text{s.t.} & & Ax \\leq b\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.array([[-3, 0], [-2, -4], [-2, -5]])\n",
    "b = np.array([-6, -10, -8])\n",
    "c = np.array([50, 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     con: array([], dtype=float64)\n",
      "     fun: 220.0\n",
      " message: 'Optimization terminated successfully.'\n",
      "     nit: 2\n",
      "   slack: array([0. , 0. , 3.5])\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([2. , 1.5])\n"
     ]
    }
   ],
   "source": [
    "sol = linprog(c, A_ub = A, b_ub = b)\n",
    "print(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value is $220$ cents when buying $2$ brownies and $1.5$ cheesecake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the Lagrangian as: \n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\mathbf{x}, \\lambda)&  = f(\\mathbf{x}) + \\lambda^\\top \\mathbf{g}(\\mathbf{x}) \\\\\n",
    "& = \\mathbf{c}^\\top \\mathbf{x} + \\lambda^\\top (A \\mathbf{x} - \\mathbf{b}) \\\\ \n",
    "& = 50x_1 + 80x_2 + \\lambda_1 (-3x_1 + 6) + \\lambda_2 (-2x_1 -4x_2 + 10) + \\lambda_3 (-2x_1 -5x_2 + 8)\n",
    "\\end{align} \n",
    "\n",
    "The optimality conditions are:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\lambda) & = \\nabla_{\\mathbf{x}}f(\\mathbf{x}) + \\lambda^\\top \\nabla_{\\mathbf{x}} \\mathbf{g}(\\mathbf{x}) \\\\ \n",
    "& = \\begin{pmatrix} 50 \\\\ 80 \\end{pmatrix} + \\lambda_1 \\begin{pmatrix}-3 \\\\ 0 \\end{pmatrix} + \\lambda_2 \\begin{pmatrix}-2 \\\\ -4 \\end{pmatrix} + \\lambda_3 \\begin{pmatrix}-2 \\\\ -5 \\end{pmatrix} \\\\\n",
    "& = \\begin{pmatrix}50 - 3\\lambda_1 - 2\\lambda_2 - 2\\lambda_3 \\\\ 80 - 4\\lambda_2 - 5\\lambda_3\\end{pmatrix} = \\mathbf{0} \\\\ \\\\\n",
    "\\lambda \\circ (A\\mathbf{x} - \\mathbf{b})& = \\begin{pmatrix} \\lambda_1 (-3x_1 + 6) \\\\ \\lambda_2 (-2x_1 -4x_2 + 10) \\\\ \\lambda_3 (-2x_1 -5x_2 + 8) \\end{pmatrix} \\\\ & = \\begin{pmatrix} \\lambda_1 \\cdot 0 \\\\ \\lambda_2 \\cdot 0 \\\\ \\lambda_3 \\cdot -3.5 \\end{pmatrix} = \\mathbf{0} & \\text{(complementary slackness)}\n",
    "\\end{align}\n",
    "\n",
    "This gives us $\\lambda_3 = 0$, and the system: $\\begin{cases}3\\lambda_1 & + & 2\\lambda_2 & = 50 \\\\ & & 4\\lambda_2 & = 80 \\end{cases}$ which yields $\\lambda_1 = \\frac{10}{3}$ and $\\lambda_2 = 20$.\n",
    "\n",
    "This means that the $2^{nd}$ constraint (corresponding to sugar quantity) is the one affecting the price the most, and changing its value will affect the price the most. The $1^{st}$ constraint has some effect but much smaller than the $2^{nd}$, and the last constraint has practically no effect, since satisfying the above 2 constraints will make it satisfied automatically, and thus changing the requirement of cream cheese a bit won't affect the total price.\n",
    "\n",
    "The optimization problem in canonical form is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{x} \\min & & 50x_1 + 80 x_2 \\\\ \n",
    "& \\text{s.t.} & & 3x_1 - 6 \\geq 0 \\\\\n",
    "& & & 2x_1 + 4x_2 - 10\\geq 0 \\\\ \n",
    "& & & 2x_1 + 5x_2 - 8 \\geq 0\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
